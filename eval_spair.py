import argparse
import torch
from torch.nn import functional as F
from tqdm import tqdm
import numpy as np
from src.flux.feat_flux import Featurizer4Eval
import os
import json
from PIL import Image
import torch.nn as nn
from einops import rearrange
import time
from torchvision.transforms import PILToTensor, ToPILImage

import warnings

warnings.filterwarnings('ignore')

import numpy as np
from scipy.spatial.distance import cosine

def main(args):
    for arg in vars(args):
        value = getattr(args,arg)
        if value is not None:
            print('%s: %s' % (str(arg),str(value)))

    torch.cuda.set_device(0)

    dataset_path = args.dataset_path
    test_path = 'PairAnnotation/test'
    json_list = os.listdir(os.path.join(dataset_path, test_path))
    all_cats = os.listdir(os.path.join(dataset_path, 'JPEGImages'))
    cat2json = {}

    for cat in all_cats:
        cat_list = []
        for i in json_list:
            if cat in i:
                cat_list.append(i)
        cat2json[cat] = cat_list

    # get test image path for all cats
    cat2img = {}
    for cat in all_cats:
        cat2img[cat] = []
        cat_list = cat2json[cat]
        for json_path in cat_list:
            with open(os.path.join(dataset_path, test_path, json_path)) as temp_f:
                data = json.load(temp_f)
                temp_f.close()
            src_imname = data['src_imname']
            trg_imname = data['trg_imname']
            if src_imname not in cat2img[cat]:
                cat2img[cat].append(src_imname)
            if trg_imname not in cat2img[cat]:
                cat2img[cat].append(trg_imname)

    if args.dit_model == 'flux':
        dit_model = Featurizer4Eval(cat_list=all_cats[:], ensemble_size=args.ensemble_size)
    else:
        raise Exception("model must be in [flux] ")

    print("saving all test images' features...")
    os.makedirs(args.save_path, exist_ok=True)
    
    # detailed caption generated by pretrained MLLM. Bring about 0.3% gain for flux
    with open("spair_detailed_captions.json") as temp_f:
        captions = json.load(temp_f)
    
    for cat in tqdm(all_cats):
        output_dict = {}
        ada_dict = {}
        
        image_list = cat2img[cat]
        for image_path in image_list:
            
            img = Image.open(os.path.join(dataset_path, 'JPEGImages', cat, image_path))
            
            ###preprocess 
            
            image_arr = np.array(img)
            in_h, in_w = image_arr.shape[:2]
            scale = args.img_size[0] / max(in_h, in_w)
            H = int(round(in_h * scale / 16)) * 16  # 保证是16的倍数
            W = int(round(in_w * scale / 16)) * 16
            img_size = (W, H)
            img = img.resize(img_size)
            img_tensor = (PILToTensor()(img) / 255.0 - 0.5) * 2
            
            
            caption = captions[cat+image_path]
            
            output_dict[image_path], ada_dict[image_path] = dit_model.forward(args, 
                                                                        img_tensor,
                                                                        caption=caption,
                                                                        category=cat,
                                                                        timestep=args.t,
                                                                        block_idx=args.k,
                                                                        ensemble_size=args.ensemble_size)
            
            output_dict[image_path], ada_dict[image_path] = output_dict[image_path].cpu(), ada_dict[image_path].cpu()
        
        torch.save(output_dict, os.path.join(args.save_path, f'{cat}.pth'))
        torch.save(ada_dict, os.path.join(args.save_path, f'{cat}_ada.pth'))
    
    total_pck = []
    all_correct = 0
    all_total = 0
    
    
    #### layernorm of our adaln for dit feature, 3072 is feature dimension of flux.
    pre_norm = nn.LayerNorm(3072, elementwise_affine=False, eps=1e-6)
    
        
    mean_image_sum=0
    mean_point_sum=0
    
    result={"image":{},"point":{}}
    
    print("Category numbers: %s"%len(all_cats))
    for cat in all_cats:
        cat_list = cat2json[cat]
        #### load data feature
        output_dict = torch.load(os.path.join(args.save_path, f'{cat}.pth'))
        ada_dict = torch.load(os.path.join(args.save_path, f'{cat}_ada.pth'))
        
        cat_pck = []
        cat_correct = 0
        cat_total = 0
        
        for cat_idx, json_path in enumerate(tqdm(cat_list)):
            
            ##load image pair
            with open(os.path.join(dataset_path, test_path, json_path)) as temp_f:
                data = json.load(temp_f)

            src_img_size = data['src_imsize'][:2][::-1]
            trg_img_size = data['trg_imsize'][:2][::-1]
            
            # B,C,H,W = 
            src_ft_raw = output_dict[data['src_imname']].cuda()
            B,C,H,W = src_ft_raw.shape
            src_ada = ada_dict[data['src_imname']].cuda()
            
            # Channel discard
            # We suppress Massive Activations (MAs) in DiT features by discarding their channels, 
            # preventing LayerNorm from propagating their adverse influence to the remaining dimensions. 
            # For a given DiT, the MA dimensions are fixed and easy to identify; we simply zero those channels.
            if args.cd:
                src_ft_raw[:,154,:,:]=0.0
                src_ft_raw[:,1446,:,:]=0.0
            
            src_ft = rearrange(src_ft_raw, "b c h w -> b (h w) c")
            src_ft = pre_norm(src_ft)
            src_ft = rearrange(src_ft, "b (h w) c -> b c h w", h=H, w=W)
            src_ft_raw = src_ft.clone()
            
            src_shift = src_ada[0][0]
            src_scale = src_ada[0][1]
            
            src_shift = src_shift.unsqueeze(0).unsqueeze(2).unsqueeze(3)
            src_scale = src_scale.unsqueeze(0).unsqueeze(2).unsqueeze(3)
            src_ft = (1 + src_scale) * src_ft + src_shift
            
            
            trg_ft_raw = output_dict[data['trg_imname']].cuda()
            B,C,H,W = trg_ft_raw.shape
            trg_ada = ada_dict[data['trg_imname']].cuda()
            
            
            # Channel discard
            if args.cd:
                trg_ft_raw[:,154,:,:]=0.0
                trg_ft_raw[:,1446,:,:]=0.0
            
            trg_ft = rearrange(trg_ft_raw, "b c h w -> b (h w) c")
            trg_ft = pre_norm(trg_ft)
            trg_ft = rearrange(trg_ft, "b (h w) c -> b c h w", h=H, w=W)
            trg_ft_raw = trg_ft.clone()
            
            trg_shift = trg_ada[0][0].unsqueeze(0).unsqueeze(2).unsqueeze(3)
            trg_scale = trg_ada[0][1].unsqueeze(0).unsqueeze(2).unsqueeze(3)
            trg_ft = (1 + trg_scale) * trg_ft + trg_shift
            
            src_ft = src_ft.to(torch.float16)
            B, C, H, W = src_ft.shape
            trg_ft = trg_ft.to(torch.float16)
            
                
            src_ft = nn.Upsample(size=src_img_size, mode='bilinear')(src_ft)
            trg_ft = nn.Upsample(size=trg_img_size, mode='bilinear')(trg_ft)
            
            
            h = trg_ft.shape[-2]
            w = trg_ft.shape[-1]

            trg_bndbox = data['trg_bndbox']
            threshold = max(trg_bndbox[3] - trg_bndbox[1], trg_bndbox[2] - trg_bndbox[0])

            total = 0
            correct = 0
            src_list = []
            trg_list = []
            
            # print(len(data['src_kps']))
            for idx in range(len(data['src_kps'])):
                total += 1
                cat_total += 1
                all_total += 1
                src_point = data['src_kps'][idx]
                trg_point = data['trg_kps'][idx]
                src_list.append(src_point)
                num_channel = src_ft.size(1)
                src_vec = src_ft[0, :, src_point[1], src_point[0]].view(1, num_channel) # 1, C
                trg_vec = trg_ft.view(num_channel, -1).transpose(0, 1) # HW, C
                src_vec = F.normalize(src_vec).transpose(0, 1) # c, 1
                trg_vec = F.normalize(trg_vec) # HW, c
                
                cos_map = torch.mm(trg_vec, src_vec).view(h, w).cpu().numpy() # H, W

                max_yx = np.unravel_index(cos_map.argmax(), cos_map.shape)
                trg_list.append([max_yx[1], max_yx[0]])
                dist = ((max_yx[1] - trg_point[0]) ** 2 + (max_yx[0] - trg_point[1]) ** 2) ** 0.5
                if (dist / threshold) <= 0.1:
                    correct += 1
                    cat_correct += 1
                    all_correct += 1

            cat_pck.append(correct / total)
            
            # gc.collect()
            torch.cuda.empty_cache()
        
        
        total_pck.extend(cat_pck)
        
        mean_image_sum = mean_image_sum + np.mean(cat_pck) * 100
        
        mean_point_sum = mean_point_sum + cat_correct / cat_total * 100
        
        
        print(f'{cat} per image PCK@0.1: {np.mean(cat_pck) * 100:.2f}')
        print(f'{cat} per point PCK@0.1: {cat_correct / cat_total * 100:.2f}')
        
        result['image'][cat] = round(np.mean(cat_pck) * 100, 2)
        result['point'][cat] = round(cat_correct / cat_total * 100, 2)
        
    print(f'All per image PCK@0.1: {np.mean(total_pck) * 100:.2f}')
    print(f'All per point PCK@0.1: {all_correct / all_total * 100:.2f}')
    
    print(f'Mean per image PCK@0.1: {mean_image_sum / len(all_cats):.2f}')
    print(f'Mean per point PCK@0.1: {mean_point_sum / len(all_cats):.2f}')
    
    result['image']["All"] = round(np.mean(total_pck) * 100, 2)
    result['point']["All"] = round(all_correct / all_total * 100, 2)
    
    result['image']["Mean"] = round(mean_image_sum / len(all_cats), 2)
    result['point']["Mean"] = round(mean_point_sum / len(all_cats), 2)
    
    
    # 判断目录是否存在
    save_dir = 'results_spair/%s'%args.dit_model
    if not os.path.exists(save_dir):
        # 如果目录不存在，则创建它
        os.makedirs(save_dir)
    # print(result)
    with open('layers_cat/%s/t%s_b%s_e%s.json'%(args.dit_model, args.t, args.k, args.ensemble_size), 'w+') as json_file:
        json.dump(result, json_file, indent=4, ensure_ascii=False)


if __name__ == "__main__":
    # print("test")
    parser = argparse.ArgumentParser(description='SPair-71k Evaluation Script')
    parser.add_argument('--dataset_path', type=str, default='/dataset/SPair-71k', help='path to spair dataset')
    parser.add_argument('--dataset', type=str, default='SPair', help='path to spair dataset')
    parser.add_argument('--save_path', type=str, default='/scratch/lt453/spair_ft/', help='path to save features')
    parser.add_argument('--dit_model', choices=['flux'], default='flux', help="which dit version to use")
    parser.add_argument('--img_size', nargs='+', type=int, default=[768, 768],
                        help='''in the order of [width, height], resize input image
                            to [w, h] before fed into diffusion model, if set to 0, will
                            stick to the original input size. by default is 768x768.''')
    parser.add_argument('--t', default=260, type=int, help='t for diffusion') ###调参[1,1000]
    parser.add_argument('--k', nargs='+', type=int, default=[28], help='which dit block to extract the ft map') ###调参[0,57]
    parser.add_argument('--ensemble_size', default=8, type=int, help='ensemble size for getting an image ft map')
    parser.add_argument("--cd", action="store_true", default=False, help='whether adopt channel discard.')
    args = parser.parse_args()
    
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True
    # print(args)
    main(args)